---
title: 阿里巴巴-菜鸟算法工程师面试
tags:
  - 面试
  - 阿里巴巴
categories: 日常
date: 2018-07-31 10:47:48
updated: 2018-07-31 10:47:48
description: 基础的重要性
keyword:
---

阿里一面

- 线性回归损失函数？LR的损失函数？线性回归损失函数的两种损失函数的比较？最大似然在逻辑回归当中的应用？梯度下降在逻辑回归中的应用？
- 随机森林 过拟合问题？ OOB？

随机森林的特征选择、数据选择都有一定的随机性，所以本身不易过拟合。

更多参见：https://blog.csdn.net/zhufenglonglove/article/details/51785220

- 决策树 决策树如何避免过拟合

先剪枝的方法

有多种不同的方式可以让决策树停止生长，下面介绍几种停止决策树生长的方法：

限制决策树的高度和叶子结点处样本的数目

1.定义一个高度，当决策树达到该高度时就可以停止决策树的生长，这是一种最为简单的方法；

2.达到某个结点的实例具有相同的特征向量，即使这些实例不属于同一类，也可以停止决策树的生长。这种方法对于处理数据中的数据冲突问题非常有效；

3.定义一个阈值，当达到某个结点的实例个数小于该阈值时就可以停止决策树的生长；

4.定义一个阈值，通过计算每次扩张对系统性能的增益，并比较增益值与该阈值的大小来决定是否停止决策树的生长。


1)REP方法是一种比较简单的后剪枝的方法，在该方法中，可用的数据被分成两个样例集合：一个训练集用来形成学习到的决策树，一个分离的验证集用来评估这个决策树在后续数据上的精度，确切地说是用来评估修剪这个决策树的影响。这个方法的动机是：即使学习器可能会被训练集中的随机错误和巧合规律所误导，但验证集合不大可能表现出同样的随机波动。所以验证集可以用来对过度拟合训练集中的虚假特征提供防护检验。

该剪枝方法考虑将书上的每个节点作为修剪的候选对象，决定是否修剪这个结点有如下步骤组成：

1：删除以此结点为根的子树

2：使其成为叶子结点

3：赋予该结点关联的训练数据的最常见分类

4：当修剪后的树对于验证集合的性能不会比原来的树差时，才真正删除该结点

因为训练集合的过拟合，使得验证集合数据能够对其进行修正，反复进行上面的操作，从底向上的处理结点，删除那些能够最大限度的提高验证集合的精度的结点，直到进一步修剪有害为止(有害是指修剪会减低验证集合的精度)。

REP是最简单的后剪枝方法之一，不过由于使用独立的测试集，原始决策树相比，修改后的决策树可能偏向于过度修剪。这是因为一些不会再测试集中出现的很稀少的训练集实例所对应的分枝在剪枝过如果训练集较小，通常不考虑采用REP算法。

- hashmap的实现、hashtable
- 快排的实现原理？ 时间复杂度nlogn怎么来的
- TF-IDF的原理是？ TF和IDF的计算公式？idf中为什么要取对数？
- GBDT与XGBoost的区别？


- 随机森林过拟合问题？ OOB？


- 决策树有哪几种？基尼系数计算公式？



| 树种类 | 划分属性依据 | 公式 |
| --- | --- | --- |
| ID3 | 信息熵 | ![](https://ws2.sinaimg.cn/large/801b780agy1ftt36spv9wj205f01kmx3.jpg) ![](http://orkqx44nq.bkt.clouddn.com/15330230324346.jpg)
 |
| C4.5 | 增益率 | ![](https://ws2.sinaimg.cn/large/801b780agy1ftt37g1tv4j20ko08aq3k.jpg) |
| CART | 基尼系数 |  ![](https://ws1.sinaimg.cn/large/801b780agy1ftt37v0l0sj20t40j2mz2.jpg)|



- LDA解释一下？推导公式？ LDA主题个数如何确定？

主题个数：https://blog.csdn.net/yt71656/article/details/50016067

1.用perplexity-topic number曲线
2.用topic_number-logP(w|T)曲线
3.计算topic之间的相似度  其中第三节提出一个定理：当主题结构的平均相似度最小时，对应的模型最优。
4. 利用HDP(层次狄利克雷过程)




- Bagging（bootstrap aggregating）和Boosting的区别

Bagging，Boosting二者之间的区别

Bagging和Boosting的区别：

1）样本选择上：

Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。

Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。

2）样例权重：

Bagging：使用均匀取样，每个样例的权重相等

Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。

3）预测函数：

Bagging：所有预测函数的权重相等。

Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。

4）并行计算：

Bagging：各个预测函数可以并行生成

Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。

# 总结

不要被带崩节奏，面试心态要好，努力把自己会的部分展示出来，不会的东西要加以避免，对自己不足的部分要加以解释，不能听之任之。

对于提到的算法，一定要很熟悉，公式的推导，核心思想，如何运用，

要思考面试官喜欢考察什么


# 参考

- http://www.cnblogs.com/liuwu265/p/4690486.html

